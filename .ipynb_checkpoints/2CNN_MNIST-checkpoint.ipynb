{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"green\">  Optimization for Mnist <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import _pickle as cPickle\n",
    "from random import randint\n",
    "from math import ceil, sqrt\n",
    "from scipy.misc import imsave\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 50\n",
    "imagesize = 28\n",
    "colors = 1\n",
    "\n",
    "img = tf.placeholder(\"float\",shape=[None,imagesize,imagesize,colors])\n",
    "lbl = tf.placeholder(\"float\",shape=[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The reference is scratch_lec04.ipynb from https://stat.columbia.edu/~cunningham/teaching/\n",
    "def conv(x, W):\n",
    "    \"\"\"simple wrapper for tf.nn.conv2d\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool(x):\n",
    "    \"\"\"simple wrapper for tf.nn.max_pool with stride size 2\"\"\"\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "def norm(x): \n",
    "    \"\"\"simple wrapper for tf.nn.lrn... See section 3.3 of Krizhevsky 2012 for details\"\"\"\n",
    "    return tf.nn.lrn(x, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The reference is scratch_lec04.ipynb from https://stat.columbia.edu/~cunningham/teaching/\n",
    "\n",
    "def compute_logits(x,pkeep):    \n",
    "    # 2 layer cnn\n",
    "    # imput image data\n",
    "    img = tf.reshape(x,shape=[-1,imagesize,imagesize,colors])# batch, then width, height, channels\n",
    "    \n",
    "    # convolution 1\n",
    "    W_conv1 = tf.get_variable('W_conv1', shape=[5,5,colors,32])\n",
    "    b_conv1 = tf.get_variable('b_conv1', shape=[32])\n",
    "    h_conv1 = tf.nn.relu(tf.add(conv(img, W_conv1), b_conv1))\n",
    "    \n",
    "    # pool 1\n",
    "    h_pool1 = maxpool(h_conv1)\n",
    "\n",
    "    # convolution 2\n",
    "    W_conv2 = tf.get_variable('W_conv2', shape=[5,5,32,64])\n",
    "    b_conv2 = tf.get_variable('b_conv2', shape=[64])\n",
    "    h_conv2 = tf.nn.relu(tf.add(conv(h_pool1, W_conv2), b_conv2))\n",
    "\n",
    "    # pool 2\n",
    "    h_pool2 = maxpool(h_conv2)\n",
    "\n",
    "    # fc 1\n",
    "    pool2flat = tf.reshape(h_pool2, [-1, int((imagesize/4) * (imagesize/4) *64)])\n",
    "    W_fc1 = tf.get_variable('W_fc1', shape=[int((imagesize/4) * (imagesize/4) * 64), 1024])\n",
    "    b_fc1 = tf.get_variable('b_fc1', shape=[1024])\n",
    "    h_fc1 = tf.nn.relu(tf.add(tf.matmul(pool2flat, W_fc1), b_fc1))\n",
    "\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, pkeep)\n",
    "    \n",
    "    # fc 2\n",
    "    W_fc2 = tf.get_variable('W_fc2', shape=[1024,10])\n",
    "    b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "\n",
    "    # logits\n",
    "    logits = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2, name='logits')\n",
    "\n",
    "    return logits, W_conv1, W_conv2,b_conv1,b_conv2, W_fc1, b_fc1, W_fc2, b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The reference is scratch_lec04.ipynb from https://stat.columbia.edu/~cunningham/teaching/\n",
    "\n",
    "def compute_cross_entropy(logits, y):\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    numerical_instability_example = 0\n",
    "    if numerical_instability_example:\n",
    "        y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "        cross_ent = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=[1]))\n",
    "    else:\n",
    "        sm_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits, name='cross_ent_terms')\n",
    "        cross_ent = tf.reduce_mean(sm_ce, name='cross_ent')\n",
    "    return cross_ent\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "rawimage =  mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_train = np.reshape(rawimage,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrain the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0: training accuracy 0.1360\n",
      "Step 200: training accuracy 0.8970\n",
      "Step 400: training accuracy 0.9340\n",
      "Step 600: training accuracy 0.9490\n",
      "Step 800: training accuracy 0.9620\n",
      "Step 1000: training accuracy 0.9690\n",
      "Step 1200: training accuracy 0.9690\n",
      "Step 1400: training accuracy 0.9750\n",
      "Step 1600: training accuracy 0.9740\n",
      "Step 1800: training accuracy 0.9670\n",
      "Step 2000: training accuracy 0.9780\n"
     ]
    }
   ],
   "source": [
    "# The reference is scratch_lec04.ipynb from https://stat.columbia.edu/~cunningham/teaching/\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32, [None, 28*28*1], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "    \n",
    "    with tf.name_scope('model'):\n",
    "        logits,W_conv1, W_conv2,b_conv1,b_conv2,W_fc1, b_fc1, W_fc2, b_fc2= compute_logits(x,pkeep)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "        opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.Session() as sess:        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(2001):\n",
    "            batch_index = np.random.randint(0,50000,batchsize)\n",
    "            X_batch = X_train[batch_index,:,:,:].reshape([batchsize,-1])\n",
    "            y_batch = [Y_train[i] for i in batch_index]\n",
    "\n",
    "            # run\n",
    "            _  = sess.run((train_step),feed_dict={x: X_batch, y: y_batch, pkeep:0.85})\n",
    "\n",
    "            # print diagnostics\n",
    "            if i%200 == 0:\n",
    "                X_batch = X_train[0:1000,:,:,:].reshape([1000,-1])\n",
    "                y_batch = Y_train[0:1000]\n",
    "                (train_error,train_logits) = sess.run((accuracy,logits), {x: X_batch, y: y_batch, pkeep:0.85})\n",
    "                print(\"\\rStep {0:3d}: training accuracy {1:0.4f}\".format(i, train_error), flush=True)\n",
    "                \n",
    "        conv1_w,conv2_w,conv1_b,conv2_b,fc1_w,fc1_b,fc2_w,fc2_b=sess.run((W_conv1,W_conv2,b_conv1,b_conv2,W_fc1, b_fc1, W_fc2, b_fc2), {x: X_train.reshape([X_train.shape[0],-1]), y: Y_train, pkeep:0.85})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the results of each layer for a given image dataset\n",
    "\n",
    "img = tf.placeholder(\"float\",[None,28,28,1])\n",
    "\n",
    "# convolution 1\n",
    "conv1_output = tf.nn.relu(tf.add(conv(img, conv1_w), conv1_b))\n",
    "\n",
    "# pool 1\n",
    "pool1_output = maxpool(conv1_output)\n",
    "\n",
    "# convolution 2\n",
    "conv2_output = tf.nn.relu(tf.add(conv(pool1_output, conv2_w), conv2_b))\n",
    "\n",
    "# pool 2\n",
    "pool2_output = maxpool(conv2_output)\n",
    "\n",
    "# fc 1\n",
    "pool2flat_output = tf.reshape(pool2_output, [-1, int(7 * 7 *64)])\n",
    "fc1_output = tf.nn.relu(tf.add(tf.matmul(pool2flat_output, fc1_w), fc1_b))\n",
    "fc1_drop_output = tf.nn.dropout(fc1_output, 0.85)\n",
    "\n",
    "# logits\n",
    "logits_output = tf.add(tf.matmul(fc1_drop_output, fc2_w), fc2_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"green\">  Method 1.Deconvolution <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the deconvolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The unpool_1 function is referred from https://github.com/kvfrans/feature-visualization/blob/master/main.py\n",
    "def unpool(value):\n",
    "    \"\"\"N-dimensional version of the unpooling operation from\n",
    "    https://www.robots.ox.ac.uk/~vgg/rg/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf\n",
    "    :param value: A Tensor of shape [b, d0, d1, ..., dn, ch]\n",
    "    :return: A Tensor of shape [b, 2*d0, 2*d1, ..., 2*dn, ch]\n",
    "    \"\"\"\n",
    "    sh = list(value.shape)\n",
    "    dim = len(sh[1:-1])\n",
    "    out = (tf.reshape(value, [-1] + sh[-dim:]))\n",
    "    for i in range(dim, 0, -1):\n",
    "        out = tf.concat([out, out],i)\n",
    "    out_size = [-1] + [(s*2) for s in sh[1:-1]] + [sh[-1]]\n",
    "    out = tf.reshape(out, out_size)\n",
    "    with tf.Session() as sess:\n",
    "        outs=out.eval()\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Take images into grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Referenced from https://github.com/InFoCusp/tf_cnnvis/blob/master/tf_cnnvis/utils.py\n",
    "def convert_into_grid(Xs, ubound=255.0, padding=1):\n",
    "    \"\"\"\n",
    "    Convert 4-D numpy array into a grid image\n",
    "    :param Xs: \n",
    "        A numpy array of images to make grid out of it\n",
    "    :type Xs: 4-D numpy array (first axis contations an image)\n",
    "    :param ubound: \n",
    "        upperbound for a image pixel value\n",
    "    :type ubound: float (Default = 255.0)\n",
    "    :param padding: \n",
    "        padding size between grid cells\n",
    "    :type padding: int (Default = 1)\n",
    "    :return: \n",
    "        A grid of input images \n",
    "    :rtype: 3-D numpy array\n",
    "    \"\"\"\n",
    "    (N, H, W, C) = Xs.shape\n",
    "    grid_size = int(ceil(sqrt(N)))\n",
    "    grid_height = H * grid_size + padding * (grid_size - 1)\n",
    "    grid_width = W * grid_size + padding * (grid_size - 1)\n",
    "    grid = np.zeros((grid_height, grid_width, C))\n",
    "    next_idx = 0\n",
    "    y0, y1 = 0, H\n",
    "    for y in range(grid_size):\n",
    "        x0, x1 = 0, W\n",
    "        for x in range(grid_size):\n",
    "            if next_idx < N:\n",
    "                grid[y0:y1, x0:x1] = Xs[next_idx]\n",
    "                next_idx += 1\n",
    "            x0 += W + padding\n",
    "            x1 += W + padding\n",
    "        y0 += H + padding\n",
    "        y1 += H + padding\n",
    "    return grid.astype('float32')\n",
    "\n",
    "def images_to_grid(images):\n",
    "    \"\"\"\n",
    "    Convert a list of arrays of images into a list of grid of images\n",
    "    :param images: \n",
    "        a list of 4-D numpy arrays(each containing images)\n",
    "    :type images: list\n",
    "    :return: \n",
    "        a list of grids which are grid representation of input images\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    grid_images = []\n",
    "    if len(images) > 0:\n",
    "        N = len(images)\n",
    "        H, W, C = images[0][0].shape\n",
    "        for j in range(len(images[0])):\n",
    "            tmp = np.zeros((N, H, W, C))\n",
    "            for i in range(N):\n",
    "                tmp[i] = images[i][j]\n",
    "            grid_images.append(np.expand_dims(convert_into_grid(tmp), axis = 0))\n",
    "    return grid_images\n",
    "\n",
    "def write_deconv(images,layer,path_outdir,filename):\n",
    "    grid_images = images_to_grid(images)\n",
    "\n",
    "    path_out = os.path.join(path_outdir, layer.lower().replace(\"/\", \"_\"))\n",
    "\n",
    "    for i in range(len(grid_images)):\n",
    "        image_data=grid_images[i]\n",
    "        resized_images=tf.image.resize_nearest_neighbor(image_data,size=[168,168])\n",
    "        with tf.Session() as sess:\n",
    "            resized_image_data=resized_images.eval()\n",
    "        grid_image_path = os.path.join(path_out)\n",
    "        os.mkdir(grid_image_path)\n",
    "        if image_data.shape[-1] == 1:\n",
    "            imsave(os.path.join(grid_image_path,filename+\".png\"),resized_image_data[0,:,:,0], format = \"png\")\n",
    "        else:\n",
    "            imsave(os.path.join(grid_image_path,filename+\".png\"), resized_image_data[0], format = \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. Display the best images for each feature map in grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def display(value, optimize_method):\n",
    "    print(\"displaying\")\n",
    "    best_feature_maps_1=[]\n",
    "    best_base_images_1=[]\n",
    "    best_feature_maps_2=[]\n",
    "    best_base_images_2=[]\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Deconv 1\n",
    "    # Referenced from https://github.com/kvfrans/feature-visualization/blob/master/main.py\n",
    "    featuresReLu1 = tf.placeholder(\"float\",[None,28,28,32])\n",
    "    unReLu1 = tf.nn.relu(featuresReLu1)\n",
    "    unBias1 = unReLu1\n",
    "    unConv1 = tf.nn.conv2d_transpose(unBias1, conv1_w, output_shape=[value.shape[0],imagesize,imagesize,colors] , \n",
    "                                strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "    # Deconv 2\n",
    "    featuresReLu2 = tf.placeholder(\"float\",[None,14,14,64])\n",
    "    unReLu2 = tf.nn.relu(featuresReLu2)\n",
    "    unBias2 = unReLu2\n",
    "    unConv2 = tf.nn.conv2d_transpose(unBias2, conv2_w, output_shape=[value.shape[0],int(imagesize/2),int(imagesize/2),32] , \n",
    "                                strides=[1,1,1,1], padding=\"SAME\")\n",
    "     \n",
    "    with tf.Session() as sess:\n",
    "        pool1_result=pool1_output.eval(feed_dict={img:value})\n",
    "        pool2_result=pool2_output.eval(feed_dict={img:value})\n",
    "    \n",
    "    # Unpool 1\n",
    "    unpool1=unpool(pool1_result)\n",
    "\n",
    "    # Unpool 2\n",
    "    unpool2=unpool(pool2_result)\n",
    "\n",
    "    # Display layer 1\n",
    "    # Referenced from https://github.com/kvfrans/feature-visualization/blob/master/main.py\n",
    "    for i in range(32):\n",
    "        isolated = unpool1.copy()\n",
    "        isolated[:,:,:,:i] = 0\n",
    "        isolated[:,:,:,i+1:] = 0\n",
    "        # Set all pixels of all feature maps except for i'th to 0\n",
    "        # We compute the sum or variance of all pixels in each feature map for all images and choose the image with the highest one\n",
    "        # This was inspired by the article https://github.com/kvfrans/feature-visualization/blob/master/main.py\n",
    "        if optimize_method==\"argmax_sum\":\n",
    "            totals = np.sum(isolated,axis=(1,2,3))\n",
    "        # This was inspired by the article https://www.sciencedirect.com/science/article/pii/S1877050917323840\n",
    "        elif optimize_method==\"argmax_var\":\n",
    "            totals = np.sum(isolated,axis=(1,2,3))\n",
    "\n",
    "        best = np.argmax(totals,axis=0)\n",
    "        \n",
    "        print(\"layer1_feature\"+str(i))\n",
    "        print(best)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        with tf.Session() as sess:\n",
    "            pixelactive = unConv1.eval(feed_dict={featuresReLu1: isolated})\n",
    "            # Deconvolution of conv 1 of all images\n",
    "\n",
    "        pb = pixelactive[best]\n",
    "        pb = pb.reshape([1, 28, 28,1]) \n",
    "        best_feature_maps_1.append(pb)\n",
    "        \n",
    "        ib = value[best]\n",
    "        ib = ib.reshape([1, 28, 28,1])\n",
    "        best_base_images_1.append(ib)\n",
    "    \n",
    "    # Show best reconstructed images for all feature maps.\n",
    "    write_deconv(best_base_images_1,\"pool1\",\"./saveimage/minst_layer1/best_base_images\",\"pool1_feature_maps\")\n",
    "    # Show all initial images corresponding to reconstructed images above.\n",
    "    write_deconv(best_feature_maps_1,\"pool1\",\"./saveimage/minst_layer1/best_feature_maps\",\"pool1_feature_maps\") \n",
    "    \n",
    "        \n",
    "    #Display layer 2\n",
    "    for j in range(64):\n",
    "        isolated2 = unpool2.copy()\n",
    "        isolated2[:,:,:,:j] = 0\n",
    "        isolated2[:,:,:,j+1:] = 0\n",
    "        if optimize_method==\"argmax_sum\":\n",
    "            totals = np.sum(isolated2,axis=(1,2,3))\n",
    "        elif optimize_method==\"argmax_var\":\n",
    "            totals = np.sum(isolated2,axis=(1,2,3))\n",
    "        best = np.argmax(totals,axis=0)\n",
    "        print(best)\n",
    "        print(\"layer2_feature\"+str(j))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        with tf.Session() as sess:\n",
    "            unConv2_active=unConv2.eval(feed_dict={featuresReLu2: isolated2})\n",
    "            pixelactive = unConv1.eval(feed_dict={featuresReLu1: unpool(unConv2_active)})\n",
    "        \n",
    "        pb_2 = pixelactive[best]\n",
    "        pb_2 = pb_2.reshape([1, 28, 28,1]) \n",
    "    \n",
    "        ib_2 = value[best]\n",
    "        ib_2 = ib_2.reshape([1, 28, 28,1])\n",
    "        best_feature_maps_2.append(pb_2)\n",
    "        best_base_images_2.append(ib_2)\n",
    "        \n",
    "    write_deconv(best_feature_maps_2,\"pool2\",\"./saveimage/minst_layer2/best_feature_maps\",\"pool2_feature_maps\")\n",
    "    write_deconv(best_base_images_2,\"pool2\",\"./saveimage/minst_layer2/best_base_images\",\"pool2_feature_maps\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "#display(X_train[np.random.choice(range(X_train.shape[0]),1),:,:,:], \"argmax_var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display(X_train[np.random.choice(range(X_train.shape[0]),500,replace=False),:,:,:],  \"argmax_var\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <font color=\"green\">  Method2. Optimization <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Channel/feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the optimized feature maps in conv1\n",
    "def optimization_minst_channel_conv1(steps=500,optimizer_method=\"SGD\"):\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_feature_conv1_1=[]\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(32):\n",
    "        \n",
    "        # We calculate the variance of all pixels of each feature map as the activation function.\n",
    "        # This was inspired by the article https://www.sciencedirect.com/science/article/pii/S1877050917323840\n",
    "        goal = tf.reduce_sum(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))[:,:,:,i:i+1],[0,1,2,3])\n",
    "        goal2 = tf.reshape(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))[:,:,:,i:i+1], [-1])\n",
    "        goal3 = tf.reduce_sum(tf.square(goal2 - (goal/784)))\n",
    "       \n",
    "        if optimizer_method==\"SGD\":\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        if optimizer_method==\"Adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        # We use 2 methods of optimizer.\n",
    "        train_step = optimizer.minimize(-1 * goal3)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "            xlast = xx.eval()\n",
    "            print(i)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "            xlast2 = xlast.reshape(1, 28, 28, 1)\n",
    "            opt_feature_conv1_1.append(xlast2)\n",
    "            \n",
    "    write_deconv(opt_feature_conv1_1,\"conv1\",\"./optimization/minst/channel\",\"conv1_feature_maps\")\n",
    "    \n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#optimization_minst_channel_conv1(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the optimized feature maps in pool1\n",
    "\n",
    "def optimization_minst_channel_pool1(steps=500,optimizer_method=\"SGD\"):\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_feature_pool_1=[]\n",
    "    start_time = time.time()\n",
    "    for i in range(32):\n",
    "        \n",
    "        goal = tf.reduce_sum(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b)))[:,:,:,i:i+1],[0,1,2,3])\n",
    "        goal2 = tf.reshape(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b)))[:,:,:,i:i+1], [-1])\n",
    "        goal3 = tf.reduce_sum(tf.square( goal2 - (goal/6272)))\n",
    "        \n",
    "        if optimizer_method==\"SGD\":\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        if optimizer_method==\"Adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        train_step = optimizer.minimize(-1 * goal3)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "            xlast = xx.eval()\n",
    "            print(i)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2 = xlast.reshape(1, 28, 28, 1)\n",
    "            opt_feature_pool_1.append(xlast2)\n",
    "            \n",
    "    write_deconv(opt_feature_pool_1,\"pool1\",\"./optimization/minst/channel\",\"pool1_feature_maps\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_channel_pool1(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the optimized feature maps in conv2\n",
    "\n",
    "def optimization_minst_channel_conv2(steps=300,optimizer_method=\"SGD\"):\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_feature_conv2=[]\n",
    "    start_time = time.time()\n",
    "    for i in range(64):\n",
    "        \n",
    "        goal = tf.reduce_sum(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b))[:,:,:,i:i+1],[0,1,2,3])\n",
    "        goal2 = tf.reshape(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b))[:,:,:,i:i+1], [-1])\n",
    "        goal3 = tf.reduce_sum(tf.square( goal2 - (goal/12544)))\n",
    "        \n",
    "        if optimizer_method==\"SGD\":\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        if optimizer_method==\"Adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_step = optimizer.minimize(-1 * goal3)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "\n",
    "            xlast = xx.eval()\n",
    "            print(i)        \n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2 = xlast.reshape([1, 28, 28, 1])\n",
    "            opt_feature_conv2.append(xlast2)\n",
    "\n",
    "    write_deconv(opt_feature_conv2,\"conv2\",\"./optimization/minst/channel\",\"conv2_feature_maps\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_channel_conv2(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the optimized feature maps in pool2\n",
    "\n",
    "def optimization_minst_channel_pool2(steps=300,optimizer_method=\"SGD\"):\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_feature_pool2=[]\n",
    "    start_time = time.time()\n",
    "    for i in range(64):\n",
    "        \n",
    "        goal = tf.reduce_sum( maxpool(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b)))[:,:,:,i:i+1],[0,1,2,3])\n",
    "        goal2 = tf.reshape( maxpool(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b)))[:,:,:,i:i+1], [-1])\n",
    "        goal3 = tf.reduce_sum(tf.square( goal2 - (goal/3136)))\n",
    "        \n",
    "        if optimizer_method==\"SGD\":\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        if optimizer_method==\"Adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_step = optimizer.minimize(-1 * goal3)\n",
    "        with tf.Session() as sess:\n",
    "            xx.initializer.run()\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "\n",
    "            xlast = xx.eval()\n",
    "            print(i) \n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2 = xlast.reshape([1, 28, 28, 1])\n",
    "            opt_feature_pool2.append(xlast2)\n",
    "\n",
    "    write_deconv(opt_feature_pool2,\"pool2\",\"./optimization/minst/channel\",\"pool2_feature_maps\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_channel_pool2(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the images that mostly activate conv1 layer\n",
    "\n",
    "def optimization_minst_layer_conv1(steps=500,optimizer_method=\"SGD\"):\n",
    "    start_time = time.time()\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_layer_conv1=[]\n",
    "    \n",
    "    # We calculate the variance of all pixels of each layer as the activation function.\n",
    "    goal = tf.reduce_sum(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))[:,:,:,:],[0,1,2,3])\n",
    "    goal2 = tf.reshape(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))[:,:,:,:], [-1])\n",
    "    goal3 = tf.reduce_sum(tf.square( goal2 - (goal/25088)))\n",
    "    \n",
    "\n",
    "    if optimizer_method==\"SGD\":\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    if optimizer_method==\"Adam\":\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_step = optimizer.minimize(-1 * goal3)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        for i in range(4):\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "\n",
    "            xlast = xx.eval()\n",
    "            print(i)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2 = xlast.reshape([1, 28, 28, 1])\n",
    "            opt_layer_conv1.append(xlast2)\n",
    "            \n",
    "    write_deconv(opt_layer_conv1,\"conv1\",\"./optimization/minst/layer\",\"conv1_layer_maps\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_layer_conv1(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the images that mostly activate pool1 layer\n",
    "\n",
    "def optimization_minst_layer_pool1(steps=500,optimizer_method=\"SGD\"):\n",
    "    start_time = time.time()\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_layer_pool1=[]\n",
    "    goal = tf.reduce_sum(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b)))[:,:,:,:],[0,1,2,3])\n",
    "    goal2 = tf.reshape(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))[:,:,:,:], [-1])\n",
    "    goal3 = tf.reduce_sum(tf.square( goal2 - (goal/6272)))\n",
    "    \n",
    "    if optimizer_method==\"SGD\":\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    if optimizer_method==\"Adam\":\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_step = optimizer.minimize(-1 * goal3)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        for i in range(4):\n",
    "            ss.run(tf.global_variables_initializer())\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "\n",
    "            xlast = xx.eval()\n",
    "            print(i)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2=xlast.reshape(28,28)\n",
    "            opt_layer_pool1.append(xlast2)\n",
    "\n",
    "    write_deconv(opt_layer_pool1,\"pool1\",\"./optimization/minst/layer\",\"pool1_layer_maps\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_layer_pool1(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the images that mostly activate conv2 layer\n",
    "\n",
    "def optimization_minst_layer_conv2(steps=300,optimizer_method=\"SGD\"):\n",
    "    start_time = time.time()\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_layer_conv2=[]\n",
    "    \n",
    "    goal = tf.reduce_sum(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b))[:,:,:,:],[0,1,2,3])\n",
    "    goal2 = tf.reshape(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b))[:,:,:,:], [-1])\n",
    "    goal3 = tf.reduce_sum(tf.square( goal2 - (goal/12544)))\n",
    "    \n",
    "    if optimizer_method==\"SGD\":\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    if optimizer_method==\"Adam\":\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(-1 * goal3)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        for i in range(4):\n",
    "            ss.run(tf.global_variables_initializer())\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "\n",
    "            xlast = xx.eval()\n",
    "            print(i)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2=xlast.reshape(28,28)\n",
    "            opt_layer_conv2.append(xlast2)\n",
    "    \n",
    "    write_deconv(opt_layer_conv2,\"conv2\",\"./optimization/minst/layer\",\"conv2_layer_maps\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_layer_conv2(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the images that mostly activate pool2 layer\n",
    "\n",
    "def optimization_minst_layer_pool2(steps=300,optimizer_method=\"SGD\"):\n",
    "    start_time = time.time()\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_layer_pool2=[]\n",
    "    goal = tf.reduce_sum( maxpool(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b)))[:,:,:,:],[0,1,2,3])\n",
    "    goal2 = tf.reshape( maxpool(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b)))[:,:,:,:], [-1])\n",
    "    goal3 = tf.reduce_sum(tf.square( goal2 - (goal/3136)))\n",
    "    if optimizer_method==\"SGD\":\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    if optimizer_method==\"Adam\":\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(-1 * goal3)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        for i in range(4):\n",
    "            ss.run(tf.global_variables_initializer())\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "\n",
    "            xlast = xx.eval()\n",
    "            print(i)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2=xlast.reshape(28,28)\n",
    "            opt_layer_pool2.append(xlast2)\n",
    "    \n",
    "    write_deconv(opt_layer_pool2,\"pool2\",\"./optimization/minst/layer\",\"pool2_layer_maps\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_layer_pool2(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the images that mostly activate fc1 layer\n",
    "\n",
    "def optimization_minst_layer_fc1(steps=300,optimizer_method=\"SGD\"):\n",
    "    start_time = time.time()\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_layer_fc1=[]\n",
    "    \n",
    "    pool2_output = maxpool(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b)))\n",
    "    pool2flat_output = tf.reshape(pool2_output, [-1, int(7 * 7 *64)])\n",
    "    goal = tf.reduce_sum(tf.nn.relu(tf.add(tf.matmul(pool2flat_output, fc1_w), fc1_b))[:,:],[0,1])\n",
    "    goal2 = tf.reshape(tf.nn.relu(tf.add(tf.matmul(pool2flat_output, fc1_w), fc1_b))[:,:], [-1])\n",
    "    goal3 = tf.reduce_sum(tf.square( goal2 - (goal/1024)))\n",
    "    \n",
    "    if optimizer_method==\"SGD\":\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    if optimizer_method==\"Adam\":\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(-1 * goal3)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        for i in range(4):\n",
    "            ss.run(tf.global_variables_initializer())\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "\n",
    "            xlast = xx.eval()\n",
    "            print(i)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2=xlast.reshape(28,28)\n",
    "            opt_layer_fc1.append(xlast2)\n",
    "    \n",
    "    write_deconv(opt_layer_fc1,\"fc1\",\"./optimization/minst/layer\",\"fc1_layer_maps\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_layer_fc1(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the image that mostly activate the logits\n",
    "\n",
    "def optimization_minst_logits(steps=500,optimizer_method=\"SGD\"):\n",
    "    start_time = time.time()\n",
    "    xx = tf.Variable(tf.random_uniform([1,28,28,1], minval=0.0, maxval=1.0, dtype=tf.float32, name=\"xx\"))  \n",
    "    opt_layer_logits=[]\n",
    "    for i in range(10):\n",
    "        \n",
    "        pool2_output = maxpool(tf.nn.relu(tf.add(conv(maxpool(tf.nn.relu(tf.add(conv(xx, conv1_w), conv1_b))), conv2_w), conv2_b)))\n",
    "        pool2flat_output = tf.reshape(pool2_output, [-1, int(7 * 7 *64)])\n",
    "        fc1_output = tf.nn.relu(tf.add(tf.matmul(pool2flat_output, fc1_w), fc1_b))#1,1024\n",
    "        fc1_drop_output = tf.nn.dropout(fc1_output, 0.85)\n",
    "        goal = tf.add(tf.matmul(fc1_drop_output, fc2_w), fc2_b)[0,i:i+1]\n",
    "        # We take the logits of each calss as the activation function.\n",
    "\n",
    "        if optimizer_method==\"SGD\":\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        if optimizer_method==\"Adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "        train_step = optimizer.minimize(-1 * goal3)\n",
    "        with tf.Session() as sess:\n",
    "            xx.initializer.run()\n",
    "            for j in range(steps):\n",
    "                train_step.run()\n",
    "\n",
    "            xlast = xx.eval()\n",
    "            print(i)\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            xlast2=xlast.reshape(28,28)\n",
    "            opt_layer_logits.append(xlast2)\n",
    "    \n",
    "    write_deconv(opt_layer_logits,\"logits\",\"./optimization/minst/logits\",\"logits_layer_maps\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization_minst_logits(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
